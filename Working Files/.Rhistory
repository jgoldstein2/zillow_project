ridge.best = predict(ridge.models.train.full, s = bestlambda.ridge, newx = x[test, ])
mean(ridge.best - y.test)^2
# MSE = 0.0342155
### Question 2
# 2.1
lassoModels = glmnet(x, y, alpha = 1, lambda = grid)
# coef(lassoModels)
plot(lassoModels, xvar = "lambda", label = TRUE, main = "Lasso Regression")
set.seed(0)
cvLasso.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 1, nfolds = 10)
plot(cvLasso.out, main = "Lasso Regression\n")
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
# lambda: 0.0367838
log(bestlambda.lasso)
lasso.bestlambda.train = predict.cv.glmnet(cvLasso.out, s ="lambda.min", newx = x[test, ])
mean((lasso.bestlambda.train - y.test)^2)
# Test MSE: .506
lassoModels.train.full = glmnet(x, y, alpha = 1, lambda = bestlambda.lasso)
lassoModels.train.full$beta
# get rid of lcp
lasso.best = predict(lassoModels.train.full, s = bestlambda.lasso, newx = x[test, ])
mean(lasso.best - y.test)^2
# MSE: .0373
# Final MSE for ridge is smaller than Lasso, so I would choose Ridge
library(DT)
library(data.table)
prostate <- read.table('[06] Prostate.txt', sep = ' ')
View(prostate)
set.seed(0)
x <- model.matrix(lpsa ~ ., prostate)[,-1]
y <- prostate$lpsa
train =  sample(1:nrow(x), 8*nrow(x)/10)
test = (-train)
y.test = y[test]
### 1.2
library(glmnet)
grid = 10^seq(5, -2, length = 100)
length(grid)
ridgeModels = glmnet(x, y, alpha = 0, lambda = grid)
# coef(ridgeModels)
### 1.3
plot(ridgeModels, xvar = "lambda", label = TRUE, main = "Ridge Regression")
# all models converge near 0 around 5 log lambda
### 1.4
set.seed(0)
cvRidge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
### 1.5
plot(cvRidge.out, main = "Ridge Regression\n")
### 1.6
bestlambda.ridge = cvRidge.out$lambda.min
bestlambda.ridge
### 1.7
ridge.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.bestlambda.train = predict(ridge.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambda.train - y.test)^2)
# Test MSE = .4913108
### 1.8
ridgeModels.train.full = glmnet(x, y, alpha = 0, lambda = bestlambda.ridge)
ridgeModels.train.full$beta
### 1.9
ridge.best = predict(ridge.models.train.full, s = bestlambda.ridge, newx = x[test, ])
mean(ridge.best - y.test)^2
# MSE = 0.0342155
### Question 2
# 2.1
lassoModels = glmnet(x, y, alpha = 1, lambda = grid)
# coef(lassoModels)
plot(lassoModels, xvar = "lambda", label = TRUE, main = "Lasso Regression")
set.seed(0)
cvLasso.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 1, nfolds = 10)
plot(cvLasso.out, main = "Lasso Regression\n")
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
# lambda: 0.0367838
log(bestlambda.lasso)
lasso.bestlambda.train = predict.cv.glmnet(cvLasso.out, s ="lambda.min", newx = x[test, ])
mean((lasso.bestlambda.train - y.test)^2)
# Test MSE: .506
lassoModels.train.full = glmnet(x, y, alpha = 1, lambda = bestlambda.lasso)
lassoModels.train.full$beta
# get rid of lcp
lasso.best = predict(lassoModels.train.full, s = bestlambda.lasso, newx = x[test, ])
mean(lasso.best - y.test)^2
# MSE: .0373
# Final MSE for ridge is smaller than Lasso, so I would choose Ridge
library(DT)
library(data.table)
prostate <- read.table('[06] Prostate.txt', sep = ' ')
View(prostate)
set.seed(0)
x <- model.matrix(lpsa ~ ., prostate)[,-1]
y <- prostate$lpsa
train =  sample(1:nrow(x), 8*nrow(x)/10)
test = (-train)
y.test = y[test]
### 1.2
library(glmnet)
grid = 10^seq(5, -2, length = 100)
length(grid)
ridgeModels = glmnet(x, y, alpha = 0, lambda = grid)
# coef(ridgeModels)
### 1.3
plot(ridgeModels, xvar = "lambda", label = TRUE, main = "Ridge Regression")
# all models converge near 0 around 5 log lambda
### 1.4
set.seed(0)
cvRidge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
### 1.5
plot(cvRidge.out, main = "Ridge Regression\n")
### 1.6
bestlambda.ridge = cvRidge.out$lambda.min
bestlambda.ridge
### 1.7
ridge.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.bestlambda.train = predict(ridge.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambda.train - y.test)^2)
# Test MSE = .4913108
### 1.8
ridgeModels.train.full = glmnet(x, y, alpha = 0, lambda = bestlambda.ridge)
ridgeModels.train.full$beta
### 1.9
ridge.best = predict(ridgeModels.train.full, s = bestlambda.ridge, newx = x[test, ])
mean(ridge.best - y.test)^2
# MSE = 0.0342155
### Question 2
# 2.1
lassoModels = glmnet(x, y, alpha = 1, lambda = grid)
# coef(lassoModels)
plot(lassoModels, xvar = "lambda", label = TRUE, main = "Lasso Regression")
set.seed(0)
cvLasso.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 1, nfolds = 10)
plot(cvLasso.out, main = "Lasso Regression\n")
bestlambda.lasso = cvLasso.out$lambda.min
bestlambda.lasso
# lambda: 0.0367838
log(bestlambda.lasso)
lasso.bestlambda.train = predict.cv.glmnet(cvLasso.out, s ="lambda.min", newx = x[test, ])
mean((lasso.bestlambda.train - y.test)^2)
# Test MSE: .506
lassoModels.train.full = glmnet(x, y, alpha = 1, lambda = bestlambda.lasso)
lassoModels.train.full$beta
# get rid of lcp
lasso.best = predict(lassoModels.train.full, s = bestlambda.lasso, newx = x[test, ])
mean(lasso.best - y.test)^2
# MSE: .0373
# Final MSE for ridge is smaller than Lasso, so I would choose Ridge
ridge.best = predict(ridgeModels.train.full, s = bestlambda.ridge, newx = x[test, ])
mean(ridge.best - y.test)^2
library(DT)
library(data.table)
prostate <- read.table('[06] Prostate.txt', sep = ' ')
View(prostate)
set.seed(0)
x <- model.matrix(lpsa ~ ., prostate)[,-1]
y <- prostate$lpsa
train =  sample(1:nrow(x), 8*nrow(x)/10)
test = (-train)
y.test = y[test]
### 1.2
library(glmnet)
grid = 10^seq(5, -2, length = 100)
length(grid)
ridgeModels = glmnet(x, y, alpha = 0, lambda = grid)
# coef(ridgeModels)
### 1.3
plot(ridgeModels, xvar = "lambda", label = TRUE, main = "Ridge Regression")
# all models converge near 0 around 5 log lambda
### 1.4
set.seed(0)
cvRidge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
### 1.5
plot(cvRidge.out, main = "Ridge Regression\n")
### 1.6
bestlambda.ridge = cvRidge.out$lambda.min
bestlambda.ridge
### 1.7
ridge.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.bestlambda.train = predict(ridge.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambda.train - y.test)^2)
# Test MSE = .4913108
### 1.8
ridgeModels.train.full = glmnet(x, y, alpha = 0, lambda = bestlambda.ridge)
ridgeModels.train.full$beta
### 1.9
ridge.best = predict(ridgeModels.train.full, s = bestlambda.ridge, newx = x[test, ])
mean(ridge.best - y.test)^2
# MSE = 0.0342155
### Question 2
# 2.1
lassoModels = glmnet(x, y, alpha = 1, lambda = grid)
# coef(lassoModels)
plot(lassoModels, xvar = "lambda", label = TRUE, main = "Lasso Regression")
set.seed(0)
cvLasso.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 1, nfolds = 10)
plot(cvLasso.out, main = "Lasso Regression\n")
bestlambda.lasso = cvLasso.out$lambda.min
bestlambda.lasso
# lambda: 0.0367838
log(bestlambda.lasso)
lasso.bestlambda.train = predict.cv.glmnet(cvLasso.out, s ="lambda.min", newx = x[test, ])
mean((lasso.bestlambda.train - y.test)^2)
# Test MSE: .506
lassoModels.train.full = glmnet(x, y, alpha = 1, lambda = bestlambda.lasso)
lassoModels.train.full$beta
# get rid of lcp
lasso.best = predict(lassoModels.train.full, s = bestlambda.lasso, newx = x[test, ])
mean(lasso.best - y.test)^2
# MSE: .0373
# Final MSE for ridge is smaller than Lasso, so I would choose Ridge
load("~/Desktop/Desktop/DataScience/NYCDSA/Projects/04.Zillow_Kaggle_Comp/Data/cleanProperties_final.Rda")
load("~/Desktop/Desktop/DataScience/NYCDSA/Projects/04.Zillow_Kaggle_Comp/team_git_hub/zillow_project/cleanTraining_final.Rda")
library(caret)
library(data.table)
library(dplyr)
library(DT)
# load('cleanTraining_final.Rda')
cleanTraining = data.table(cleanTraining)
###############################################################
# Feature Engineering and Selection
###############################################################
cleanTraining$valueratioNF = cleanTraining$taxvaluedollarcnt / cleanTraining$taxamount
cleanTraining$assessValueMetricNF = cleanTraining$structuretaxvaluedollarcnt / cleanTraining$landtaxvaluedollarcnt
cleanTraining$livingareapropNF = cleanTraining$calculatedfinishedsquarefeet / cleanTraining$lotsizesquarefeet
cleanTraining$totalroomNF = cleanTraining$bathroomcnt + cleanTraining$bedroomcnt
cols_drop <- c("bedroomcnt", "regionidzip", "taxvaluedollarcnt", "taxamount",
"taxdelinquencyflag", "deckflag", "unitcnt", "month", "property_group",
"regionidcounty")
cleanTraining <- cleanTraining[ , !(names(cleanTraining) %in% cols_drop)]
###############################################################
# Machine Learning Preparation
###############################################################
# Partition the training and test data (75% train, 25% test) on month:
set.seed(0)
trainIndex <- createDataPartition(cleanTraining$totalroomNF,
p = .75,
list = FALSE,
times = 1)
# training set
subTrain <- cleanTraining[ trainIndex,-1]
## testing set
subTest  <- cleanTraining[-trainIndex,-1]
# full training set
fullTrain = cleanTraining[,-1]
## define metric - MAE
maeSummary <- function(data, lev = NULL, model = NULL) {
mae_score <- sum(abs(data$obs - data$pred)) / nrow(data)
names(mae_score) <- "MAE"
mae_score
}
###############################################################
# Cross Validation
###############################################################
gridSearch <- trainControl(method = "cv",
number = 5,
summaryFunction = maeSummary,
verboseIter = TRUE)
gbmGrid <-  expand.grid(interaction.depth = c(20, 24, 28),
n.trees = c(2000, 3500, 5000),
shrinkage = c(.0001),
n.minobsinnode = 10)
set.seed(0)
gbmFit2 <- train(logerror ~ .,
data = subTrain,
method = "gbm",
preProcess = c("center", "scale"),
metric = "MAE",
maximize = FALSE,
tuneGrid = gbmGrid,
trControl = gridSearch,
verbose = TRUE)
## visualize parameters
plot(gbmFit2)
## best parameters
gbmFit2$bestTune
## variable importance
gbmImp <- varImp(gbmFit2, scale = FALSE)
plot(gbmImp, top = 8)
###############################################################
# Valid Model with SubTest
###############################################################
results <- data.frame(obs = subTest$logerror,
pred = predict(gbmFit2, newdata = subTest))
maeSummary(results)
cor(results)
###############################################################
# Refit Model with Full Training Set
###############################################################
# Get best parameters for model on full training set
gbmFit2$bestTune
fitBestModel <- trainControl(method = "none",
summaryFunction = maeSummary)
gbmFit3 <- train(logerror ~ .,
data = fullTrain,
method = "gbm",
preProcess = c("center", "scale"),
metric = "MAE",
maximize = FALSE,
trControl = fitBestModel,
tuneGrid = gbmFit2$bestTune,
verbose = TRUE)
#Make sure new data doesn't have parcelid
predict(gbmFit3, newdata = fullTrain)
## visualize parameters
plot(gbmFit3)
## best parameters
gbmFit3$bestTune
## variable importance
gbmImp2 <- varImp(gbmFit3, scale = FALSE)
plot(gbmImp2, top = 8)
###############################################################
# Load Properties File and Add/Drop Features Used in Train Set
###############################################################
# load('cleanProperties_final.Rda')
###This must be updated if any new features are added or dropped!
cleanProperties$valueratioNF = cleanProperties$taxvaluedollarcnt / cleanProperties$taxamount
cleanProperties$assessValueMetricNF = cleanProperties$structuretaxvaluedollarcnt / cleanProperties$landtaxvaluedollarcnt
cleanProperties$livingareapropNF = cleanProperties$calculatedfinishedsquarefeet / cleanProperties$lotsizesquarefeet
cleanProperties$totalroomNF = cleanProperties$bathroomcnt + cleanProperties$bedroomcnt
cols_drop <- c("bedroomcnt", "regionidzip", "taxvaluedollarcnt", "taxamount",
"taxdelinquencyflag", "deckflag", "unitcnt", "month", "property_group",
"regionidcounty")
cleanProperties <- cleanProperties[ , !(names(cleanProperties) %in% cols_drop)]
###############################################################
# Make Prediction for Submission
###############################################################
makePrediction <- function(model, newdata, months, labels) {
predictions <- cleanProperties[, "parcelid", drop=FALSE]
for(i in 1:length(months)) {
cleanProperties$month <- months[i]
predictions[, labels[i]] <- predict(model, newdata = cleanProperties)
}
write.csv(x = predictions, file = "submission.csv",
quote = FALSE, row.names = FALSE)
return(predictions)
}
makePrediction(gbmFit3, newdata = cleanProperties, months = c(10, 11, 12, 22, 23, 24),
labels = c("201610", "201611", "201612", "201710", "201711", "201712"))
load("~/Desktop/Desktop/DataScience/NYCDSA/Projects/04.Zillow_Kaggle_Comp/team_git_hub/zillow_project/cleanTraining_final.Rda")
load("~/Desktop/Desktop/DataScience/NYCDSA/Projects/04.Zillow_Kaggle_Comp/Data/cleanProperties_final.Rda")
setwd("~/Desktop/Desktop/DataScience/NYCDSA/Projects/04.Zillow_Kaggle_Comp/Data/Submissions")
cleanTraining$totalroomNF = cleanTraining$bathroomcnt + cleanTraining$bedroomcnt
library(caret)
library(data.table)
library(dplyr)
library(DT)
# load('cleanTraining_final.Rda')
cleanTraining = data.table(cleanTraining)
###############################################################
# Feature Engineering and Selection
###############################################################
cleanTraining$valueratioNF = cleanTraining$taxvaluedollarcnt / cleanTraining$taxamount
cleanTraining$assessValueMetricNF = cleanTraining$structuretaxvaluedollarcnt / cleanTraining$landtaxvaluedollarcnt
cleanTraining$livingareapropNF = cleanTraining$calculatedfinishedsquarefeet / cleanTraining$lotsizesquarefeet
cleanTraining$totalroomNF = cleanTraining$bathroomcnt + cleanTraining$bedroomcnt
cols_drop <- c("bedroomcnt", "regionidzip", "taxvaluedollarcnt", "taxamount",
"taxdelinquencyflag", "deckflag", "unitcnt", "month", "property_group",
"regionidcounty")
cleanTraining <- cleanTraining[ , !(names(cleanTraining) %in% cols_drop)]
###############################################################
# Machine Learning Preparation
###############################################################
# Partition the training and test data (75% train, 25% test) on month:
set.seed(0)
trainIndex <- createDataPartition(cleanTraining$totalroomNF,
p = .75,
list = FALSE,
times = 1)
# training set
subTrain <- cleanTraining[ trainIndex,-1]
## testing set
subTest  <- cleanTraining[-trainIndex,-1]
# full training set
fullTrain = cleanTraining[,-1]
## define metric - MAE
maeSummary <- function(data, lev = NULL, model = NULL) {
mae_score <- sum(abs(data$obs - data$pred)) / nrow(data)
names(mae_score) <- "MAE"
mae_score
}
###############################################################
# Cross Validation
###############################################################
gridSearch <- trainControl(method = "cv",
number = 5,
summaryFunction = maeSummary,
verboseIter = TRUE)
gbmGrid <-  expand.grid(interaction.depth = c(20, 24, 28),
n.trees = c(2000, 3500, 5000),
shrinkage = c(.0001),
n.minobsinnode = 10)
set.seed(0)
gbmFit2 <- train(logerror ~ .,
data = subTrain,
method = "gbm",
preProcess = c("center", "scale"),
metric = "MAE",
maximize = FALSE,
tuneGrid = gbmGrid,
trControl = gridSearch,
verbose = TRUE)
## visualize parameters
plot(gbmFit2)
## best parameters
gbmFit2$bestTune
## variable importance
gbmImp <- varImp(gbmFit2, scale = FALSE)
plot(gbmImp, top = 8)
###############################################################
# Valid Model with SubTest
###############################################################
results <- data.frame(obs = subTest$logerror,
pred = predict(gbmFit2, newdata = subTest))
maeSummary(results)
cor(results)
###############################################################
# Refit Model with Full Training Set
###############################################################
# Get best parameters for model on full training set
gbmFit2$bestTune
fitBestModel <- trainControl(method = "none",
summaryFunction = maeSummary)
gbmFit3 <- train(logerror ~ .,
data = fullTrain,
method = "gbm",
preProcess = c("center", "scale"),
metric = "MAE",
maximize = FALSE,
trControl = fitBestModel,
tuneGrid = gbmFit2$bestTune,
verbose = TRUE)
#Make sure new data doesn't have parcelid
predict(gbmFit3, newdata = fullTrain)
## visualize parameters
plot(gbmFit3)
## best parameters
gbmFit3$bestTune
## variable importance
gbmImp2 <- varImp(gbmFit3, scale = FALSE)
plot(gbmImp2, top = 8)
###############################################################
# Load Properties File and Add/Drop Features Used in Train Set
###############################################################
# load('cleanProperties_final.Rda')
###This must be updated if any new features are added or dropped!
cleanProperties$valueratioNF = cleanProperties$taxvaluedollarcnt / cleanProperties$taxamount
cleanProperties$assessValueMetricNF = cleanProperties$structuretaxvaluedollarcnt / cleanProperties$landtaxvaluedollarcnt
cleanProperties$livingareapropNF = cleanProperties$calculatedfinishedsquarefeet / cleanProperties$lotsizesquarefeet
cleanProperties$totalroomNF = cleanProperties$bathroomcnt + cleanProperties$bedroomcnt
cols_drop <- c("bedroomcnt", "regionidzip", "taxvaluedollarcnt", "taxamount",
"taxdelinquencyflag", "deckflag", "unitcnt", "month", "property_group",
"regionidcounty")
cleanProperties <- cleanProperties[ , !(names(cleanProperties) %in% cols_drop)]
###############################################################
# Make Prediction for Submission
###############################################################
makePrediction <- function(model, newdata, months, labels) {
predictions <- cleanProperties[, "parcelid", drop=FALSE]
for(i in 1:length(months)) {
cleanProperties$month <- months[i]
predictions[, labels[i]] <- predict(model, newdata = cleanProperties)
}
write.csv(x = predictions, file = "submission.csv",
quote = FALSE, row.names = FALSE)
return(predictions)
}
makePrediction(gbmFit3, newdata = cleanProperties, months = c(10, 11, 12, 22, 23, 24),
labels = c("201610", "201611", "201612", "201710", "201711", "201712"))
load("~/Desktop/Desktop/DataScience/NYCDSA/Projects/04.Zillow_Kaggle_Comp/team_git_hub/zillow_project/cleanTraining_final.Rda")
load("~/Desktop/Desktop/DataScience/NYCDSA/Projects/04.Zillow_Kaggle_Comp/Data/cleanProperties_final.Rda")
library(caret)
library(data.table)
library(dplyr)
library(DT)
# load('cleanTraining_final.Rda')
cleanTraining = data.table(cleanTraining)
###############################################################
# Feature Engineering and Selection
###############################################################
cleanTraining$valueratioNF = cleanTraining$taxvaluedollarcnt / cleanTraining$taxamount
cleanTraining$assessValueMetricNF = cleanTraining$structuretaxvaluedollarcnt / cleanTraining$landtaxvaluedollarcnt
cleanTraining$livingareapropNF = cleanTraining$calculatedfinishedsquarefeet / cleanTraining$lotsizesquarefeet
cleanTraining$totalroomNF = cleanTraining$bathroomcnt + cleanTraining$bedroomcnt
cols_drop <- c("bedroomcnt", "regionidzip", "taxvaluedollarcnt", "taxamount",
"taxdelinquencyflag", "deckflag", "unitcnt", "month", "property_group",
"regionidcounty")
cleanTraining <- cleanTraining[ , !(names(cleanTraining) %in% cols_drop)]
###############################################################
# Machine Learning Preparation
###############################################################
# Partition the training and test data (75% train, 25% test) on month:
set.seed(0)
trainIndex <- createDataPartition(cleanTraining$totalroomNF,
p = .75,
list = FALSE,
times = 1)
# Partition the training and test data (75% train, 25% test) on month:
set.seed(0)
trainIndex <- createDataPartition(cleanTraining$livingareapropNF,
p = .75,
list = FALSE,
times = 1)
install.packages('brnn')
library(brnn)
setwd("~/Desktop/Desktop/DataScience/NYCDSA/Projects/04.Zillow_Kaggle_Comp/team_git_hub/zillow_project/Working Files")
